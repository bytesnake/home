 > we can optimize over the convex inputs [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#we can optimize over the convex inputs)
 > sums of convex functions are also convex [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#sums of convex functions are also convex)
 > energy-based models operates [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#energy-based models operates)
 > direct differentiation of the argmin [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#direct differentiation of the argmin)
 > structured prediction energy [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#structured prediction energy)
 > general model is built over the output space [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#general model is built over the output space)
 > forward and backward passes [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#forward and backward passes)
 > specify them manually [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#specify them manually)
 > probabilistic semantics at all [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#probabilistic semantics at all)
 > optimization over the output is guaranteed [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#optimization over the output is guaranteed)
 > input and output example space [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#input and output example space)
 > imputes the likely val- [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#imputes the likely val-)
 > sums of rectified half-planes to data [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#sums of rectified half-planes to data)
 > capture complex joint models [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#capture complex joint models)
 > max-margin structured [source](/home/losch/Note/papers/stash/input-convex-nn.pdf#max-margin structured)
